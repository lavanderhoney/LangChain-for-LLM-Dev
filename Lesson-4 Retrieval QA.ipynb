{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Q&A over Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Components of the Retrieval Q&A Chain**\n",
    "\n",
    "#### **1.1 Document Store**\n",
    "The document store is where all your documents are stored and indexed for retrieval. Popular options include:\n",
    "\n",
    "- **FAISS (Facebook AI Similarity Search):** For vector-based similarity search.\n",
    "- **Pinecone:** A scalable vector database for high-performance retrieval.\n",
    "- **Weaviate or Chroma:** Modern alternatives with feature-rich capabilities.\n",
    "\n",
    "Here, we're using **DocArrayInMemorySearch**, as it is suitable for small scale applications like this one, whereas the above DBs are more suited for large-scale applications\n",
    "\n",
    "The document store allows for the efficient retrieval of documents based on vector similarity.\n",
    "\n",
    "#### **1.2 Embedding Model**\n",
    "The embedding model converts documents and user queries into dense vector representations. These embeddings capture semantic meaning and are essential for similarity searches\n",
    "\n",
    "#### **1.3 Retriever**\n",
    "The retriever is responsible for searching the document store and returning the most relevant documents based on the query embedding. Two main types of retrieval methods are used:\n",
    "\n",
    "- **Similarity-based retrieval:** Finds documents closest to the query in vector space.\n",
    "- **Hybrid retrieval:** Combines traditional keyword search with vector similarity.\n",
    "\n",
    "#### **1.4 Large Language Model (LLM)**\n",
    "The LLM interprets the retrieved documents and generates an accurate and contextually appropriate answer to the user’s query.\n",
    "\n",
    "#### **1.5 Chain Logic**\n",
    "Chains in LangChain enable the combination of multiple components into a coherent pipeline. For Retrieval Q&A, the chain typically involves:\n",
    "\n",
    "- Embedding the query.\n",
    "- Retrieving relevant documents.\n",
    "- Answer generation using the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, Andrew Ng's lesson used deprecated classes, so here I use the latest ones, as suggested by LangChain: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama, OllamaLLM, OllamaEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Document Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Loading Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "page_content='SUFIA: Language-Guided Augmented Dexterity\n",
      "for Robotic Surgical Assistants\n",
      "Masoud Moghani1, Lars Doorenbos 2, William Chung-Ho Panitch 3,\n",
      "Sean Huver4, Mahdi Azizian 4, Ken Goldberg 3, Animesh Garg 1,4,5\n",
      "Abstract— In this work, we present SUFIA , the first frame-\n",
      "work for natural language-guided augmented dexterity for\n",
      "robotic surgical assistants. SUFIA incorporates the strong\n",
      "reasoning capabilities of large language models (LLMs) with\n",
      "perception modules to implement high-level planning and low-\n",
      "level control of a robot for surgical sub-task execution. This\n",
      "enables a learning-free approach to surgical augmented dexterity\n",
      "without any in-context examples or motion primitives. SUFIA\n",
      "uses a human-in-the-loop paradigm by restoring control to\n",
      "the surgeon in the case of insufficient information, mitigating\n",
      "unexpected errors for mission-critical tasks. We evaluate SUFIA\n",
      "on four surgical sub-tasks in a simulation environment and two\n",
      "sub-tasks on a physical surgical robotic platform in the lab,\n",
      "demonstrating its ability to perform common surgical sub-tasks\n",
      "through supervised autonomous operation under challenging\n",
      "physical and workspace conditions.\n",
      "Project website: orbit-surgical.github.io/sufia\n",
      "I. I NTRODUCTION\n",
      "Recently, one prominent trend in surgery has been the\n",
      "increasing adoption of robotic surgical assistants (RSAs) in\n",
      "operating rooms. These RSAs are often controlled via local\n",
      "or remote teleoperation through a console by a trained human\n",
      "surgeon using hand controllers or other input peripherals,\n",
      "thereby enabling the surgeon to perform tasks with enhanced\n",
      "precision, dexterity, and control during an operation [ 1]. The\n",
      "teleoperated surgical procedures often involve tedious, repeti-\n",
      "tive, or time-consuming sub-tasks. Augmented dexterity in\n",
      "surgery holds the potential to simplify the surgical workflow,\n",
      "reduce surgeon fatigue, and improve patient outcomes [ 2],\n",
      "[3].\n",
      "Learning-based approaches such as reinforcement and\n",
      "imitation learning learn policies to solve specific surgical sub-\n",
      "tasks [4], [5]. However, complex, long-horizon surgical sub-\n",
      "tasks are often computationally expensive, require extensive\n",
      "domain knowledge and reward engineering, and involve\n",
      "time-consuming dataset curation. Furthermore, the lack of\n",
      "generalizability limits the utility of learning-based models in\n",
      "safety-critical applications where unseen, in-domain variations\n",
      "are prevalent. As a result, most surgical robotic platforms\n",
      "still lack any level of autonomous capabilities [6].\n",
      "In recent years, Large Language Models (LLMs) have\n",
      "received considerable attention for their ability to respond\n",
      "naturally to textual prompts and have been integrated into var-\n",
      "ious domains, including the field of robotics and autonomous\n",
      "agents [7]. Language and vision models have demonstrated\n",
      "considerable promise in long-horizon robot planning and\n",
      "1University of Toronto, 2University of Bern, 3University of California,\n",
      "Berkeley, 4NVIDIA, 5Georgia Institute of Technology\n",
      "moghani@cs.toronto.edu, animesh.garg@gatech.edu\n",
      "High-levelPlanning Low-levelCode Feedback\n",
      "LLM Generation\n",
      "RobotActionPerception\n",
      "Surgeon: \n",
      "“Please lift the needle”\n",
      "Assistant:\n",
      "“The needle has been successfully \n",
      "lifted to the correct position”\n",
      "Re-planning Autonomy Delegation\n",
      "needle\n",
      "Fig. 1: An overview of SUFIA automating the lifting of a\n",
      "suture needle from a surgical site. SUFIA receives commands\n",
      "from a surgeon in natural language and converts them to high-\n",
      "level planning and low-level control code. If a task requires object\n",
      "interaction, SUFIA queries a perception module for object state\n",
      "information and generates low-level trajectories and robot actions\n",
      "accordingly. SUFIA can assist a surgeon with open-ended tasks,\n",
      "such as moving the robot in a desired motion to help complete a\n",
      "surgical task. In times of inefficient information, SUFIA delegates\n",
      "full control back to the surgeon.\n",
      "control [ 8], [ 9], [ 10]. While these efforts still require pre-\n",
      "trained skills and motion primitives, they have demonstrated\n",
      "the potential of unified many-modality models for addressing\n",
      "a variety of complex tasks involving improved generalization\n",
      "to novel objects and unseen tasks.\n",
      "In surgical settings, LLMs have the additional potential\n",
      "to aid interaction between a human surgeon and a robot via\n",
      "natural language teleoperation. This empowers the surgeon\n",
      "with the ability to use both fine-grained manual control\n",
      "and autonomous natural language conversational control in\n",
      "commanding the RSA to perform a sub-task. This approach\n",
      "promises both more natural human-robot coordination and\n",
      "the potential for developing general-purpose models for\n",
      "autonomous surgery beyond the capability of current task-by-\n",
      "arXiv:2405.05226v1  [cs.RO]  8 May 2024' metadata={'source': 'SuFIA.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\n",
    "    file_path=\"SuFIA.pdf\",\n",
    "    extract_images=True,\n",
    "    )\n",
    "\n",
    "pages = [page for page in loader.lazy_load()]\n",
    "\n",
    "print(len(pages)) #the pdf has 8 pages, and this prints 8\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4681\n"
     ]
    }
   ],
   "source": [
    "print(len(pages[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Configuring the DB with document and embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milap\\envs\\langchain_course_env\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "db = DocArrayInMemorySearch.from_documents(pages, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the QnA chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`db.as_retriever()` wraps the vector db into a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milap\\envs\\langchain_course_env\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(db.as_retriever(), combine_docs_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What are autonomous agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous agents, also known as self-driving or intelligent agents, refer to software systems that can perceive their environment, make decisions, and take actions without human intervention. These agents use artificial intelligence (AI) and machine learning (ML) algorithms to learn from data and adapt to new situations.\n",
      "\n",
      "Autonomous agents are designed to operate in a wide range of applications, including:\n",
      "\n",
      "1. Robotics: Autonomous robots that can navigate and interact with their environment.\n",
      "2. Autonomous vehicles: Self-driving cars, drones, and trucks that can operate without human intervention.\n",
      "3. Smart homes: Intelligent systems that can control lighting, temperature, security, and entertainment systems.\n",
      "4. Healthcare: Personalized medicine, disease diagnosis, and treatment planning.\n",
      "5. Finance: Algorithmic trading, risk management, and portfolio optimization.\n",
      "\n",
      "Characteristics of autonomous agents:\n",
      "\n",
      "1. Autonomy: They operate independently, making decisions without human oversight.\n",
      "2. Intelligence: They use AI and ML to learn from data and adapt to new situations.\n",
      "3. Perception: They can sense their environment through sensors and cameras.\n",
      "4. Action: They can take actions based on their decision-making process.\n",
      "\n",
      "Types of autonomous agents:\n",
      "\n",
      "1. Simple autonomous agents: These are basic systems that can perform a specific task, such as navigation or control.\n",
      "2. Complex autonomous agents: These are sophisticated systems that can learn from data and adapt to new situations.\n",
      "3. Hybrid autonomous agents: These combine the strengths of simple and complex agents.\n",
      "\n",
      "Benefits of autonomous agents:\n",
      "\n",
      "1. Increased efficiency: Autonomous agents can perform tasks faster and more accurately than humans.\n",
      "2. Improved safety: Autonomous agents can reduce accidents and injuries by detecting potential hazards.\n",
      "3. Enhanced decision-making: Autonomous agents can analyze large amounts of data to make informed decisions.\n",
      "4. Cost savings: Autonomous agents can reduce costs by automating repetitive tasks.\n",
      "\n",
      "However, autonomous agents also raise concerns about:\n",
      "\n",
      "1. Job displacement\n",
      "2. Bias and fairness\n",
      "3. Cybersecurity risks\n",
      "4. Accountability\n",
      "\n",
      "Overall, autonomous agents have the potential to revolutionize various industries and transform the way we live and work.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'SuFIA.pdf', 'page': 5}, page_content='(N1) (N2) (N3)\\n(N4) (N5)\\nFig. 5: Needle variations in simulation. We consider five instances\\nof simulated suture needles (N1 - N5) with various sizes and shapes\\nto conduct the generalizability experiment in O RBIT -Surgical.\\nNeedle Handover – \"Pick up the needle with the arm\\nclosest to it, move it directly to the handover location between\\nthe two arms, and keep holding the needle. Grasp the right\\nside of the needle with the other robot arm, then right after\\nthat, release the needle from the first robot and stay put.\"\\nVessel Dilation – \"Grasp the vessel from its leftmost\\nside with robot 0 and pull it backward to the left by 5\\nmillimeters while holding on to it to dilate. When grasping\\nthe vessel, grasp it 15 millimeters below the left point.\"\\nShunt Insertion – \"Lift the small shunt from the middle\\nand insert it into the left opening of the large tube. Approach\\nthe large tube from the left. Only lift the tube by 8 millimeters\\nand move horizontally to insert.\"\\nE. Perception Adaptation and Domain Variation\\nWhile we envision domain-specific perception models for\\napplications of our framework in a given surgical environment\\n(e.g., a needle segmentation network in our real-world\\nphysical experimentation), here we investigate whether a\\ngeneral-purpose segmentation model, LangSAM [ 45], can\\nbe utilized by the perception module in simulated surgical\\nenvironments to enhance the generalizability of the SUFIA\\nframework to various object shapes.\\nWe found that due to the slender shape of dVRK\\narms, the use of LangSAM in the perception module\\nsometimes returned both the dVRK arm and the suture\\nneedle when prompted to find the \"needle.\" However,\\ndescriptive adjectives (e.g. \"round\" or \"small white needle\")\\nenable LangSAM to correctly identify and segment the\\nsuture needle well enough for SUFIA to generate the\\nrequired steps to grasp and lift it. GPT-4 was also able to\\nreason over the sizes of the segmented objects and properly\\ndetermine the object of interest; here is an example of the\\nLLM planner: \" there are two objects detected\\nas \"white needle,\" but only one of them\\nhas dimensions that match a needle\\n(Width: 0.011, Length: 0.032, Height:\\n0.002). The second object’s dimensions\\nare too large to be the needle we are\\ninterested in. Therefore, we will focus\\non the first object with the correct\\ndimensions.\"\\nTo study the generalizability of the perception module\\nacross various needle sizes and shapes, we conduct a study\\nwith five different needles, three needles in different sizes (N1\\nPerception Module N1 N2 N3 N4 N5\\nIsaac Sim Camera 5 / 5 4 / 5 5 / 5 5 / 5 4 / 5\\nLangSAM 4 / 5 5 / 5 4 / 5 3 / 5 3 / 5\\nTABLE II: Domain variation evaluation in simulation. We report\\nthe success rates for lifting suture needles with varied sizes and\\nirregular shapes (suture needles N1 - N5) over 5 trial runs with two\\nvariations of the perception module.\\n- N3; Fig. 5) and two irregular shapes (N4 and N5; Fig. 5).\\nTABLE II shows the performance of SUFIA to lift various\\nneedles, which is robust to their shape and size.\\nF . LLMs Investigation\\nHere, we investigate the effect of different LLMs on the\\nperformance of the needle lift task. We use the same prompt\\nand needle locations for all LLMs. As the error handling in\\nSUFIA would, in principle, allow an LLM to keep trying\\nendlessly until it generates code where no exceptions are\\nraised, we limit the number of errors to five before terminating\\nthe program.\\nNone of the open-source LLMs can perform the simple\\ntask of needle picking and have a hard time following\\nthe instructions in the prompt. All models struggle with\\nunderstanding that detect_object() will print its result\\nrather than return it as a variable in a Python script. When\\nfaced with errors, Mixtral [ 46] typically only outputs updated\\ncode snippets when asked to improve a code block rather\\nthan the whole code. CodeLlama [ 15] calls many undefined\\nfunctions, such as get_end_effector_pose(), despite\\nthe end-effector pose being given in the prompt. Llama 2 [ 47]\\nhas a variety of mistakes related to understanding the steps\\nin the task, such as forgetting to close the gripper or moving\\nit down before lifting the needle.\\nGPT3.5 Turbo similarly misunderstands\\ndetect_object(), often assigning its value to a\\nvariable called needle_position, despite the prompt\\nstating the function does not return anything. Beyond that,\\nGPT3.5 Turbo does consistently define a proper plan to\\nlift the needle, but even when it calls detect_object()\\ncorrectly, the information is not incorporated successfully.\\nAll in all, in our experiments, only GPT-4 Turbo could\\nfollow all instructions and appropriately plan and execute the\\nrelatively simple task of lifting a suture needle.\\nG. Re-planning\\nTo illustrate the benefits of our safety modules, we provide\\nan example in the Needle Lift environment in Fig. 7.\\nIn the first row, SUFIA executes the plan it came up with\\nto perform the task desired by the user, i.e., orienting its\\ngripper with the needle, moving to a position where it can\\ngrab it, and picking it up. While picking it up, we move the\\nneedle to a different position. Because SUFIA validates the\\nexpected and observed position of the objects it manipulates,\\nit correctly identifies the needle is not where it should be.\\nBased on the newly observed state, SUFIA devises a new\\nplan to proceed with the user instruction, finally lifting it to\\nthe desired height.'),\n",
       " Document(metadata={'source': 'SuFIA.pdf', 'page': 4}, page_content='1 2 3 4\\nFig. 4: Physical Needle Handover task. (1) Starting workspace configuration. The needle is placed in a fixed position within the\\nworkspace, and the gripper positions are randomized. In this stage, the SUFIA LLM planner queries for and identifies the pose of the\\nsuture needle, determines which robot arm is closest to it, and plans a trajectory for that robot arm to reach the suture needle. (2) The\\nclosest robot arm approaches and grasps the suture needle. (3) The suture needle is lifted to a neutral handover position. At this stage, the\\nSUFIA LLM planner detects the pose of the suture needle at the handover position and plans a trajectory for the second robot arm to\\napproach the needle. (4) The second robot arm descends and grasps the needle, then the first robot arm releases the needle after the second\\nrobot arm has grasped it. We provide task videos at orbit-surgical.github.io/sufia\\nFailure ModesExperiment Success Rate Planning Steps\\n(P) (E)\\nSim Experiments\\nNeedle Lift 100 % 6 0 0\\nNeedle Handover 90 % 14 - 16 1 0\\nVessel Dilation 60 % 6 - 8 3 1\\nShunt Insertion 70 % 8 - 9 3 0\\nPhysical Experiments\\nNeedle Lift 100 % 6 0 0\\nNeedle Handover 50 % 14 - 18 2 3\\nTABLE I: Evaluation Success rate and planning steps required\\nfor surgical sub-tasks automation (10 trials for each experiment).\\nFailure modes: (P) denotes planning and (E) denotes execution\\nfailures. Sim experiments are carried out in ORBIT -Surgical, a high-\\nfidelity surgical simulation framework. Physical Experiments are\\nperformed on a dVRK surgical platform.\\ntask, all planning failures were due to not rotating the\\ngrippers to grasp the vessel’s rim correctly. In the Shunt\\nInsertion task, the planning failures were from incorrect\\nlift height calculations before insertion.\\nWe observed that the performance of SUFIA was relatively\\nrobust to the more complex physics and observation spaces\\nof the physical environment, with 0 and 2 planning fail-\\nures encountered during the Needle Lift and Needle\\nHandover experiments, respectively. This aligns closely\\nwith the framework’s performance in simulation. However, we\\nfound that hysteresis and encoder mismatch within the cable-\\ndriven dVRK resulted in variation between the commanded\\nand actual gripper positions. Although SUFIA was often\\nable to recover from the failures induced by this mismatch\\nthrough its re-planning behavior, the lack of explicit servoing\\ncan result in dropping the needle during more complicated\\nhandovers.\\nD. Task Prompt Analysis\\nSimple tasks such as Needle Lift require a simple\\nprompt to function properly. The surgeon can specify a\\nposition to transfer the needle to or allow the LLM to\\ndetermine a specific lift height above the table.\\nMore sophisticated prompts are needed for tasks that re-\\nquire several steps for successful completion. In the Needle\\nHandover task, the surgeon can provide additional notes\\nfor SUFIA to consider (e.g., \"please note that for a handover,\\neach robot should grasp the needle from the side closest to\\nit.\"). The sequence in which the robot arms grasp and hand\\nover to each other, as well as the location of the handover, can\\neither be specified directly or left for the SUFIA to decide\\nbased on the distance to the needle or other environmental\\nstates.\\nThe SUFIA planner may suggest unnecessary steps that\\nmay not be required for task completion and may potentially\\nelongate task execution time. For instance, in the Vessel\\nDilation task, the vanilla prompt for dilating a vessel can\\nsometimes lead to an additional step of \"Lift the vessel slightly\\nby moving the end-effector upwards to provide clearance from\\nthe table.\" The surgeon can provide additional information\\nabout the fact that the clamps are holding the vessel vertically\\nto eliminate the suggestion of lifting steps in dilating the\\nvessel. Similarly, in the Shunt Insertion task, additional\\ninformation such as \"please lift the small tube by a specific\\namount off of the table and horizontally insert it\" helps to\\nachieve better planning and execution.\\nVision language models (VLMs) can also be incorporated\\nin SUFIA to enhance the general visual understanding of the\\nLLM planner. For instance, in the Vessel Dilation task,\\nGPT4-Vision [12] can provide the planner with environmental\\ncontext regarding the orientation of the vessel phantom. In\\nthis example, the VLM response can complement the user\\nprompt: I see a vertical yellow tube on the\\nright side of the image. It appears to\\nbe standing upright on one of its ends\\non a flat surface. While useful for providing\\ngeneral visual context, similar to [ 44], we find GPT4-Vision\\nunreliable as a standalone perception module for detecting\\n(small) objects’ spatial states and omit it for the remainder\\nof our experiments.\\nThe prompts used for the tasks are as follows:\\nNeedle Lift – \"Pick up the needle and lift it.\"'),\n",
       " Document(metadata={'source': 'SuFIA.pdf', 'page': 3}, page_content='insufficient information. Fig. 6 illustrates an instance when\\nthe system is unable to execute a command properly and\\nreturns the control to the surgeon to adjust the environment\\nor provide further instruction.\\nTogether, these two components enhance the safety and\\nreliability of the assistant, which is crucial in the domain\\nof surgical robotics. Note that the surgeon can also directly\\ninstruct SUFIA to re-take control of the robot, ensuring a\\nsmooth interplay between surgeon and robot.\\nV. E XPERIMENTAL RESULTS\\nTo empirically measure the efficacy of SUFIA, we perform\\nexperiments both in ORBIT -Surgical, a high-fidelity surgical\\nsimulation framework, and on a dVRK platform in the lab.\\nA. Experimental Setup\\nWe conduct our simulation experiments in ORBIT -\\nSurgical [ 42], which accurately imitates joint articulation\\nand low-level controllers of the real dVRK platform, sup-\\nports contact-rich physical interactions between rigid and\\ndeformable objects, and provides high-fidelity rendering.\\nFurthermore, ORBIT -Surgical provides an interface for tele-\\noperation, which enables the user to work together with\\nSUFIA to solve a sub-task if needed. We use a camera\\nsensor in NVIDIA Omniverse to acquire 512 × 512 rendered\\nRGB-D images and ground-truth semantic segmentation\\nmasks. In section V-E, we will discuss the utility of general\\nsegmentation models to adapt the workflow to objects with\\nvarious configuration in simulation.\\nPhysical experiments are performed on a da Vinci Research\\nKit (dVRK) [ 33] robot surgical assistant, using an Allied\\nVision Prosilica GC 1290 stereo camera pair for visual input.\\nThese cameras are capable of producing paired stereo frames\\nat a resolution of 1280 × 960 at 33 fps. Real-world depth\\nimages are then subsequently obtained by passing image pairs\\nthrough RAFT-Stereo RVC [43], a state-of-the-art network for\\npredicting image correspondences using optical flow, and then\\nusing the camera’s calculated intrinsic matrix to retrieve depth\\nfrom these point discrepancies. We find that this approach\\nprovides better empirical results than traditional depth cameras\\nin our use case due to the small, reflective objects and short\\nfocal lengths involved in the surgical setting. To emulate the\\nreal-world conditions encountered in a surgical setting, our\\nworkspace consists of a 3-D Med suturing tissue phantom\\non a red background. The phantom is then wrapped in blue\\ncloth to imitate the use of a surgical cover during operation.\\nThese physical experiments introduce additional challenges,\\nincluding a more challenging perception task, estimation and\\ncontrol noise, and more complex physics.\\nThroughout this section, we use GPT-4 Turbo [ 12] unless\\nstated otherwise.\\nB. Tasks and Evaluation Metrics\\nWe demonstrate the generalizability of SUFIA by evaluat-\\ning it across four distinct simulated surgical sub-tasks derived\\nfrom ORBIT -Surgical, as shown in Fig. 3. We additionally\\nselect two of the subtasks (Needle Lift and Needle Handover)\\nfor evaluation using our physical setup. Each sub-task poses\\n(a) (b)\\n(c) (d)\\nFig. 3: Surgical sub-tasks. (a) Needle Lift: lift a suture needle\\nto a desired height, (b) Needle handover: pick and handover\\na suture needle, (c) Vessel Dilation: grip the vessel rim and\\ndilate by pulling, (d) Shunt Insertion: insert a soft tube into\\nlarger vessel phantom. Best viewed in color.\\nunique challenges to show the robustness of the proposed\\nworkflow, described as follows:\\nNeedle Lift – In this task, the needle (N1 in Fig. 5)\\nis initialized at a random position and orientation within the\\nreach area from a single dVRK arm. The task is successful\\nif the robot grasps and lifts the needle to a specified height\\nabove the table.\\nNeedle Handover – This task involves transferring a\\nneedle using a dual-arm dVRK setup. The needle is initially\\npositioned randomly. The arm closest to the needle first grasps\\nand lifts it to a specified handover location. Subsequently,\\nthe second arm reaches for the needle, grasps it, and takes it\\nto a desired position. The task is successful if the needle is\\neffectively transferred from the initial to the second arm.\\nVessel Dilation – In this task, a spring clamp\\nassembly holds a soft vessel phantom from two points. The\\ndVRK arm is required to grip the vessel rim from a third point\\nfacing the robot and dilate the vessel by pulling backward. A\\nsuccessful trial is defined if the robot fully dilates the vessel.\\nShunt Insertion – This task requires using a dVRK\\ngripper to insert a shunt into a vessel phantom. The arm grasps\\nthe shunt from the middle, lifts it slightly, and then inserts\\nit into a vessel phantom. The task is considered successful\\nif, upon release by the grippers, the shunt remains inside the\\nvessel phantom.\\nC. SUFIA Evaluation\\nWe now discuss the effectiveness of SUFIA on solving\\nthe proposed surgical sub-tasks. SUFIA utilizes a perception\\nmodule to localize the objects and proposes a sequence of\\nsub-trajectories to perform the required task. We present\\nthe success rate for each sub-task for 10 trials in TABLE I.\\nOverall, SUFIA is able to solve all proposed surgical sub-\\ntasks requiring precise grasping of small surgical objects in\\nsimulation. Each task poses a unique challenge for automation,\\nincluding object-gripper alignment and executing many steps\\nto achieve successful results. In the Vessel Dilation'),\n",
       " Document(metadata={'source': 'SuFIA.pdf', 'page': 6}, page_content='SuFIA LLM Planner: To begin the task of finding the suturing site, I will first need to detect \\nthe location of the suturing site within the environment. I will call the `detect_object` function \\nwith the parameter \"suturing site\" to obtain its position.\\n```python\\ndetect_object(\"suturing site\")\\n```\\nSystem execution output: Object not found!\\nSuFIA LLM Planner: Since the \"suturing site\" object was not found in the environment, I \\nwill defer control to the surgeon to provide further instructions or to adjust the environment so \\nthat the suturing site can be detected.\\n```python\\ntransfer_control()\\n```\\nControl of the robot is transferred to the Surgeon.\\n(a) (b)\\n(c)\\nSurgeon: Please find the suturing site.\\nFig. 6: Interactive human-in-the-loop approach. (a) An overview of the environment showing the dVRK robotic arm and endoscope\\ncamera as well as a needle and a suturing pad in ORBIT -Surgical, (b) RGB image from the endoscope camera focused on the needle as\\ninput to the perception module, (c) delegation of control back to the surgeon due to the inability of the system to identify a suturing site.\\nThe uncertainty and occlusion in a surgical scene might lead to undesired object localization and retrieval. A human-in-the-loop approach\\nenables a fail-safe paradigm for interactive robotic surgical assistants.\\nLLM Function Call for Verification:\\n# Verify that the needle has been picked up and \\nlifted\\nverify_object(\"needle\", lifted_position, 0.015)\\nFunction Output: \\nverify_object output: False\\nLLM Planner: \\nThe verification step has returned `False`, indicating that \\nthe needle has not been successfully picked up and lifted \\nto the specified position. Therefore, I will need to re-open \\nthe gripper and plan a new trajectory to complete the task.\\nLLM Function Call for Verification:\\n# Verify that the needle has been picked up and \\nlifted\\nverify_object(\"needle\", lifted_position, 0.015)\\nFunction Output: \\nverify_object output: True\\n(a) Failed execution\\n(b) Re-planning and successful \\nexecution\\nFig. 7: Re-planning. (a) A failed execution as a result of not\\nfinding a proper grasping point on the needle. The LLM verification\\nstep indicates that the task was not executed successfully. (b) LLM\\nplanner proposes a new plan to re-identify and lift the needle by\\nthe robot.\\nH. Limitations\\nThe current best results are obtained with API calls to\\nGPT-4 Turbo. Due to the generation speed of OpenAI’s\\nAPI, SUFIA does not operate in real-time; for real-world\\nneedle handover trials, the API calls invoked during planning\\n(including sensing and replanning) took an average of 25.5\\nseconds to complete, out of an average total trial time of\\n61.4 seconds. However, with open-source models constantly\\nimproving, running a quantized open-source LLM on-device\\nwill soon be a viable way to improve inference time greatly.\\nMoreover, while we incorporate two measures specifically\\ndesigned to improve safety and reliability, deploying au-\\ntonomous or semi-autonomous RSAs in real-world scenarios\\nstill has the potential to bring risks from unexpected circum-\\nstances an AI system might not be able to handle.\\nVI. C ONCLUSION\\nWe present SUFIA , a modular framework for natural\\nsurgeon-robot interaction. We show that our training-free\\napproach, which uses pre-trained LLMs to provide low-level\\ncontrol of surgical robots, can successfully interact with\\nsmall surgical objects and execute surgeon commands for\\nautomating surgical sub-tasks. Safety is bolstered through\\nre-planning capabilities and a human-in-the-loop approach.\\nWe evaluate the efficacy of SUFIA for common surgical\\nsub-tasks in simulated and physical experiments in the lab\\nand show that the proposed method succeeds across different\\nsub-tasks with various difficulty levels. These results suggest\\nthat language-guided autonomy has the potential to enhance\\nsurgeon’s efficiency in surgical procedures.\\nIn future work, we plan to test the viability of quantized\\nopen-source LLMs on-device to improve inference time.\\nThis will also address any privacy concerns stemming from\\ntransmitting highly sensitive medical information to off-site\\nservers. Furthermore, we intend to explore the usefulness of\\nfine-tuned large language and vision models in S UFIA.\\nREFERENCES\\n[1] M. Hwang, D. Seita, B. Thananjeyan, J. Ichnowski, S. Paradis, D. Fer,\\nT. Low, and K. Goldberg, “Applying depth-sensing to automated\\nsurgical manipulation with a da vinci robot,” in 2020 international\\nsymposium on medical robotics (ISMR). IEEE, 2020, pp. 22–29.\\n[2] S. Sen, A. Garg, D. V . Gealy, S. McKinley, Y . Jen, and K. Goldberg,\\n“Automating multi-throw multilateral surgical suturing with a mechani-\\ncal needle guide and sequential convex optimization,” in 2016 IEEE\\ninternational conference on robotics and automation (ICRA). IEEE,\\n2016, pp. 4178–4185.\\n[3] H. Lin, B. Li, X. Chu, Q. Dou, Y . Liu, and K. W. S. Au, “End-to-\\nend learning of deep visuomotor policy for needle picking,” in 2023\\nIEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS). IEEE, 2023, pp. 8487–8494.\\n[4] P. M. Scheikl, B. Gyenes, R. Younis, C. Haas, G. Neumann, M. Wagner,\\nand F. Mathis-Ullrich, “LapGym - An open source framework for\\nreinforcement learning in robot-assisted laparoscopic surgery,” Journal\\nof Machine Learning Research, vol. 24, no. 368, pp. 1–42, 2023.\\n[5] J. Xu, B. Li, B. Lu, Y .-H. Liu, Q. Dou, and P.-A. Heng, “Surrol:\\nAn open-source reinforcement learning centered and dvrk compatible\\nplatform for surgical robot learning,” in 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS). IEEE, 2021,\\npp. 1821–1828.\\n[6] A. Attanasio, B. Scaglioni, E. De Momi, P. Fiorini, and P. Valdastri,\\n“Autonomy in surgical robotics,” Annual Review of Control, Robotics,\\nand Autonomous Systems, vol. 4, pp. 651–679, 2021.\\n[7] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y . Lin et al., “A survey on large language model\\nbased autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\\n[8] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
