{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Chains in LangChain\n",
    "\n",
    "## What is a chain in LangChain ?\n",
    "A chain is essentially a sequence of operations that takes input, processes it through various components, and generates an output. Chains are the building blocks for creating workflows that integrate language models, tools, and logic to perform tasks.\n",
    "\n",
    "### Key Features of Chains: \n",
    "1. **Modularity**: Chains consist of discrete components, like prompts, memory, tools, and other chains, that can be connected together. \n",
    "2. **Input/Output Transformation**: Chains take structured inputs (e.g., dictionaries) and return structured outputs. They abstract away the complexity of managing intermediate states.\n",
    "3. **Composability**: Chains can be nested to build more complex workflows, where the output of one chain becomes the input to another.\n",
    "4. **Integration**: Chains can integrate with external APIs, databases, or other computational tools to fetch, process, or store data.\n",
    "\n",
    "### When to Use Chains:\n",
    "* Automating workflows: For repetitive tasks, like text summarization or question answering.\n",
    "* Managing complexity: When building multi-step pipelines or applications.\n",
    "* Interactive systems: For chatbots, virtual assistants, or systems that need memory/context.\n",
    "\n",
    "\n",
    "### Outline of lesson\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain\n",
    "\n",
    "The class `LLMChain` is deprecated in the Langchain, so we have to use the pipe command: `|` to chain the prompt and the LLM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are some suggestions for a company name that makes laser guns:\\n\\n1. **LaserStrike Inc.**: This name conveys a sense of power and precision, suggesting a company that produces high-quality laser technology.\\n2. **Apex Laser Systems**: \"Apex\" implies a pinnacle of achievement, which could be fitting for a company that creates cutting-edge laser guns.\\n3. **Lumina Laser Technologies**: \"Lumina\" means light in Latin, which is fitting for a laser-based product. This name also has a futuristic and innovative feel to it.\\n4. **Pulse Laser Solutions**: \"Pulse\" suggests energy and movement, which could be appealing for a company that produces dynamic laser technology.\\n5. **Spectra Laser Systems**: \"Spectra\" refers to the range of colors or frequencies in light, which is fitting for a laser-based product. This name also has a scientific and technical feel to it.\\n6. **NovaTech Laser**: \"Nova\" means new in Latin, which could suggest innovation and progress. This name also has a strong, modern sound to it.\\n7. **LaserGuard Industries**: \"LaserGuard\" implies protection and safety, which could be an important aspect of a company that produces laser guns for various applications.\\n8. **Eon Laser Systems**: \"Eon\" means eternal or lasting in Greek mythology, which could suggest a company that creates long-lasting and reliable laser technology.\\n\\nUltimately, the best name for your company will depend on your brand identity, values, and target audience. I recommend choosing a name that reflects your company\\'s mission, products, and personality.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-12-21T16:34:06.5136963Z', 'done': True, 'done_reason': 'stop', 'total_duration': 35697306900, 'load_duration': 4675886100, 'prompt_eval_count': 41, 'prompt_eval_duration': 2148000000, 'eval_count': 333, 'eval_duration': 28859000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-26e47bff-f07c-4e33-9e79-04c75e2902fb-0', usage_metadata={'input_tokens': 41, 'output_tokens': 333, 'total_tokens': 374})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template( \"What is the best name to describe \\\n",
    "    a company that makes {product}?\")\n",
    "\n",
    "chain = prompt | llm #This is of type RunnableSequence, same as chain = RunnableSequence(first=runnable_1, last=runnable_2)\n",
    "chain.invoke(\"Lazer Gun\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['product'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['product'], input_types={}, partial_variables={}, template='What is the best name to describe     a company that makes {product}?'), additional_kwargs={})]) middle=[] last=ChatOllama(model='llama3.2', temperature=0.0)\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "print(chain)\n",
    "print(type(chain))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain/RunnableSequence\n",
    "These are linear workflows that process input through one or more steps sequentially.\\\n",
    "Note that when a task is simple, and stateless,i.e, the intermediate results are of no use, we can use a single prompt, rather than chaining it.\n",
    "\n",
    "Sequential chains are more helpful for **dynamic inputs**: If the output of one step needs to be reused, modified, or combined with other data before being passed to the next step, splitting into chains is more manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 20-word description for the company: \"LaserTech Industries - Developing cutting-edge laser guns with precision and innovation for various applications.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.sequential import SimpleSequentialChain\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
    "chain_one = first_prompt | llm\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\"Write a 20 words description for the following company:{company_name}\")\n",
    "chain_two = second_prompt | llm\n",
    "\n",
    "simple_chain = RunnableSequence(first=chain_one, last=chain_two)\n",
    "# simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "\n",
    "result = simple_chain.invoke(\"Lazer Gun\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequentialChain\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Feature                   | `SequentialChain`                       | `SimpleSequentialChain`              |\n",
    "|---------------------------|------------------------------------------|---------------------------------------|\n",
    "| **Flexibility**           | High: Supports input/output mapping.    | Low: Outputs flow linearly.          |\n",
    "| **Intermediate Outputs**  | Yes: Tracks intermediate results.       | No: Only the final result is stored. |\n",
    "| **Input/Output Control**  | Customizable per chain step.            | Automatic linear flow.               |\n",
    "| **Use Case Complexity**   | For complex, multi-step workflows.      | For simple, linear workflows.        |\n",
    "| **Ease of Use**           | Requires more setup and configuration.  | Easier to implement.                 |\n",
    "\n",
    "With SequentialChain, at each intermediate step, we can specify where this step can get its input from, doesn't have to be the immediate previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in bellow implementation, I have ommittted the use of deprecated `LLMChain`, and used `RunnableSequence`. \\\n",
    "\\\n",
    "`RunnableLambda` is essential here, since after each step in the chain, the output from the LLM is of type `AIMessage`, or `str` in my case due `StrOutputParser`. This doesn't work,\n",
    "since subsequent prompt template expect a mapped input, i.e key-value pair of input variable and its corresponding value. \\\n",
    "\\\n",
    "I've acheived that here, using `ChatPromptTemplate` object's `format_messages()`method, which I can call using a lambda function inside the LLM chain, as its wrapped with `RunnableLambda`.\\\n",
    "Then, in the middle section of `sequential_chain`, `RunnableMap` allows to explicitly map the outputs from the specified chains to a desired key, so that these outputs can be passed to the last section, and \\\n",
    "here, the \"summary\" and \"language\" outputs, are both fed in the last step. If this wasn't used, the output from middle would only contain `qsn_prompt`'s  output, due to the sequential nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Step Output: Here's the translation of the review to English:\n",
      "\n",
      "\"This Andrew Ng course is a disaster. I do not recommend it. It shattered my expectations because in every lesson, nothing is explained clearly, and on top of that, all the commands are outdated. So, I have to spend time looking up the methods from Langchain documentation for each lesson, which wastes so much time. I'm very disappointed.\"\n",
      "\n",
      "Note: The original text uses some informal language and colloquial expressions (e.g., \"Ça détruit mes attentes\", \"Je suis très déçu\") that are not typically used in formal reviews or written feedback.\n",
      "Summarization Step Output: The reviewer expresses strong disappointment with Andrew Ng's course, citing unclear explanations, outdated commands, and wasted time researching alternative methods to keep up with the material.\n",
      "Language Detection Step Output: The review appears to be written in French, as indicated by the use of French phrases such as \"Ça détruit mes attentes\" (which translates to \"It shattered my expectations\") and \"Je suis très déçu\" (which translates to \"I am very disappointed\").\n",
      "Input to follow_up: {'summary': \"The reviewer expresses strong disappointment with Andrew Ng's course, citing unclear explanations, outdated commands, and wasted time researching alternative methods to keep up with the material.\", 'language': 'The review appears to be written in French, as indicated by the use of French phrases such as \"Ça détruit mes attentes\" (which translates to \"It shattered my expectations\") and \"Je suis très déçu\" (which translates to \"I am very disappointed\").'} <class 'dict'>\n",
      "Here's a potential follow-up response:\n",
      "\n",
      "\"Je comprends pleinement vos sentiments, Monsieur [Reviewer's Name]. Les explications claires et les commandes à jour sont essentiels pour une expérience de cours éducative réussie. Il est frustrant de se retrouver perdu dans des concepts obsolètes ou tenter de trouver des alternatives pour rester au courant du matériel, ce qui peut être un processus temps-consuming et stresant. Je vous encourage à partager vos expériences avec d'autres étudiants afin qu'ils puissent prendre les précautions nécessaires et éviter les mêmes erreurs que vous avez commises.\"\n",
      "\n",
      "Translation:\n",
      "\n",
      "\"I fully understand your feelings, Mr./Ms./Mrs./Dr. [Reviewer's Name]. Clear explanations and up-to-date commands are essential for a successful educational course experience. It's frustrating to get lost in outdated concepts or try to find alternatives to stay current with the material, which can be a time-consuming and stressful process. I encourage you to share your experiences with other students so they can take precautions and avoid the same mistakes you made.\"\n",
      "\n",
      "Note: The response is written in French, using phrases such as \"Je comprends pleinement\" (I fully understand), \"Monsieur [Reviewer's Name]\" (Mr./Ms./Mrs./Dr. [Reviewer's Name]), and \"Je vous encourage\" (I encourage you) to maintain the same tone and language as the original review.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnableLambda, RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the language model\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# Define prompts\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to English:\\n\\n{Review}\"\n",
    ")\n",
    "summarize_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\"\n",
    ")\n",
    "qsn_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "followup_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow-up response to the following summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "\n",
    "# Construct the chain\n",
    "sequential_chain = RunnableSequence(\n",
    "    first=RunnableLambda( \n",
    "            lambda input_prompt: translate_prompt.format_messages(Review=input_prompt)\n",
    "        ) | llm | StrOutputParser()\n",
    "        | RunnableLambda(lambda output: print(f\"Translation Step Output: {output}\") or output),\n",
    "\n",
    "    middle=[\n",
    "        RunnableMap({\n",
    "            \"summary\": RunnableLambda( lambda output: summarize_prompt.format_messages(English_Review=output) )\n",
    "            | llm | StrOutputParser() \n",
    "            | RunnableLambda(lambda output: print(f\"Summarization Step Output: {output}\") or output),\n",
    "\n",
    "            \"language\": RunnableLambda( lambda output: qsn_prompt.format_messages(Review=output) )\n",
    "            | llm | StrOutputParser() \n",
    "            | RunnableLambda(lambda output: print(f\"Language Detection Step Output: {output}\") or output)\n",
    "        })\n",
    "    ],\n",
    "\n",
    "    last=RunnableLambda(lambda inputs: print(f\"Input to follow_up: {inputs} {type(inputs)}\") or inputs) | RunnableLambda(\n",
    "        lambda outputs: followup_prompt.format_messages(\n",
    "            summary=outputs[\"summary\"], language=outputs[\"language\"]\n",
    "        )\n",
    "    )| llm | StrOutputParser()\n",
    "\n",
    ")\n",
    "\n",
    "# Input data\n",
    "input_data = {\n",
    "    \"Review\": \"Cette course d'Andrew Ng est une désastre. Je ne recommande pas. Ça détruit mes attentes, car dans chaque leçon, rien n'est expliqué clairement, en plus, \"\n",
    "              \"tous les commandes sont obsolètes. Donc, il me faut que je dois chercher sur les docs de Langchain la propre méthode, et pour chaque leçon, ça perd du grand temps. Je suis très déçu.\"\n",
    "}\n",
    "\n",
    "# Invoke the chain\n",
    "result = sequential_chain.invoke(input_data[\"Review\"])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RouterChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from  langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the router:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\milap\\AppData\\Local\\Temp\\ipykernel_15528\\2724433427.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  router_template_str = \"\"\"Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a\n"
     ]
    }
   ],
   "source": [
    "router_template_str = \"\"\"Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a \n",
    "description of what the prompt is best suited for. You may also revise the original input if you think that revising\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
    "REMEMBER: Only give output that you are being asked for. Don't include any additional information or context or suggestions, just the output.\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{input}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the destination chains, chains where the `router_chain` will route to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics in a concise and easy to understand manner.\",\n",
    "        \"template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions.\",\n",
    "        \"template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"history\",\n",
    "        \"description\": \"Good for answering questions about history.\",\n",
    "        \"template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computerscience\",\n",
    "        \"description\": \"Good for answering coding or computer science related questions.\",\n",
    "        \"template\": computerscience_template\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics\n",
      "math\n",
      "history\n",
      "computerscience\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template=\"You are a very smart physics professor. You are great at answering questions about physics in a conciseand easy to understand manner. When you don't know the answer to a question you admitthat you don't know.\\n\\nHere is a question:\\n{input}\"), additional_kwargs={})]\n",
      "input_variables=['destinations', 'input'] input_types={} partial_variables={} template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a \\ndescription of what the prompt is best suited for. You may also revise the original input if you think that revising\\nit will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\n{destinations}\\n\\n<< INPUT >>\\n{input}\\n\\n\\n<< OUTPUT (remember to include the ```json)>>'\n"
     ]
    }
   ],
   "source": [
    "destination_prompt_templates = {}\n",
    "destinations = \"\"\n",
    "\n",
    "for prompt_info in prompt_infos:\n",
    "    name = prompt_info[\"name\"]\n",
    "    chain = ChatPromptTemplate.from_template(prompt_info[\"template\"])\n",
    "    destination_prompt_templates[name] = chain\n",
    "\n",
    "destinations = \"\\n\".join([f\"{prompt_info['name']}\"for prompt_info in prompt_infos])\n",
    "print(destinations)\n",
    "\n",
    "default_prompt_template = ChatPromptTemplate.from_template(\"You are a very smart person. You are great at answering questions in a concise and easy to understand manner. Here is a question: {input}\")\n",
    "destination_prompt_templates[\"DEFAULT\"] =  default_prompt_template\n",
    "\n",
    "router_prompt = PromptTemplate(template=router_template_str, input_variables=[\"input\",\"destinations\"])\n",
    "\n",
    "print(destination_prompt_templates[\"physics\"])\n",
    "print(router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "main_chain = RunnableSequence(\n",
    "\n",
    "    first=RunnableLambda(\n",
    "        lambda input_prompt: router_prompt.format(\n",
    "            input=input_prompt, destinations=destinations)\n",
    "    ) | RunnableLambda(lambda router_stuff: print(f\"Input to router chain: \\n{router_stuff}\\n\") or router_stuff)\n",
    "    | llm | JsonOutputParser(),\n",
    "\n",
    "    last=RunnableLambda(lambda router_stuff: print(f\"Input to destination chain: \\n{router_stuff} \\n{type(router_stuff)}\\n\") or router_stuff) |\n",
    "    RunnableLambda(  \n",
    "        # Use the destination chain if it exists, otherwise use the default chain\n",
    "        # router_outputs is a JSON object with keys \"destination\" and \"next_inputs\"\n",
    "        lambda router_outputs:\n",
    "        destination_prompt_templates[router_outputs[\"destination\"]].format_messages(\n",
    "            input=router_outputs[\"next_inputs\"]\n",
    "        )\n",
    "        if router_outputs[\"destination\"] in destination_prompt_templates else destination_prompt_templates[\"DEFAULT\"]\n",
    "    )\n",
    "    | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to router chain: \n",
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a \n",
      "description of what the prompt is best suited for. You may also revise the original input if you think that revising\n",
      "it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "physics\n",
      "math\n",
      "history\n",
      "computerscience\n",
      "\n",
      "<< INPUT >>\n",
      "What is quantum entaglement?\n",
      "\n",
      "\n",
      "<< OUTPUT (remember to include the ```json)>>\n",
      "\n",
      "Input to destination chain: \n",
      "{'destination': 'physics', 'next_inputs': 'What are the key principles of quantum entanglement and how does it relate to quantum mechanics?'} \n",
      "<class 'dict'>\n",
      "\n",
      "Quantum entanglement! One of the most fascinating and counterintuitive phenomena in all of physics.\n",
      "\n",
      "In essence, quantum entanglement is a fundamental concept in quantum mechanics that describes the interconnectedness of two or more particles. The key principles can be summarized as follows:\n",
      "\n",
      "1. **Correlation**: When two particles are entangled, their properties become correlated, meaning that the state of one particle cannot be described independently of the other.\n",
      "2. **Non-locality**: Entangled particles can be separated by arbitrary distances, and yet, their properties remain connected in a way that transcends space and time.\n",
      "3. **Quantum superposition**: Entangled particles exist in a superposition of states, meaning they can have multiple properties simultaneously.\n",
      "\n",
      "Now, here's the really interesting part: entanglement is not just a property of individual particles; it's also a fundamental aspect of quantum mechanics itself. In other words, entanglement is an inherent feature of the quantum world that cannot be explained by classical physics.\n",
      "\n",
      "Quantum mechanics predicts that entangled particles will exhibit correlations that are beyond the realm of classical explanations. For example, if you measure the spin of one particle in an entangled pair, it instantly affects the spin of the other particle, regardless of the distance between them.\n",
      "\n",
      "Entanglement has been experimentally confirmed numerous times and is a cornerstone of quantum information science, including quantum computing and cryptography.\n",
      "\n",
      "However, I must admit that there's still much to be learned about entanglement, particularly when it comes to understanding its relationship with the fundamental laws of physics. While we have a good grasp of the principles, some aspects of entanglement remain shrouded in mystery, and ongoing research is helping to shed more light on this fascinating phenomenon.\n",
      "\n",
      "Do you have any follow-up questions or would you like me to elaborate on any aspect of quantum entanglement?\n"
     ]
    }
   ],
   "source": [
    "answer = main_chain.invoke(\"What is quantum entaglement?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to router chain: \n",
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a \n",
      "description of what the prompt is best suited for. You may also revise the original input if you think that revising\n",
      "it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "physics\n",
      "math\n",
      "history\n",
      "computerscience\n",
      "\n",
      "<< INPUT >>\n",
      "What are partial differntial equations?\n",
      "\n",
      "\n",
      "<< OUTPUT (remember to include the ```json)>>\n",
      "\n",
      "Input to destination chain: \n",
      "{'destination': 'physics', 'next_inputs': 'What are partial differential equations in physics?'} \n",
      "<class 'dict'>\n",
      "\n",
      "Partial differential equations (PDEs) are a fundamental tool in physics, and I'm happy to explain them in simple terms.\n",
      "\n",
      "In essence, PDEs describe how quantities change over space and time. They're called \"partial\" because they involve rates of change with respect to multiple variables, not just one variable like we typically see in ordinary differential equations (ODEs).\n",
      "\n",
      "Think of it this way: imagine you're standing at a point on the surface of the Earth, and you want to know how temperature changes over time. You could write an ODE that describes how temperature changes as you move along a line (like a river) or as you change altitude. But what if you wanted to know how temperature changes both with respect to your position on the surface and with respect to time? That's where PDEs come in.\n",
      "\n",
      "PDEs are used to model all sorts of phenomena, from heat transfer and fluid dynamics to wave propagation and quantum mechanics. They're a powerful tool for describing complex systems that involve multiple variables and changing conditions.\n",
      "\n",
      "For example, the Schrödinger equation is a famous PDE that describes how particles like electrons behave in quantum mechanics. It's a partial differential equation because it involves both position (x) and time (t), as well as other variables like energy (E).\n",
      "\n",
      "So, to summarize: PDEs are equations that describe how quantities change over space and time, involving multiple variables and rates of change. They're a fundamental part of physics, and they help us understand all sorts of fascinating phenomena!\n"
     ]
    }
   ],
   "source": [
    "answer = main_chain.invoke(\"What are partial differntial equations?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to router chain: \n",
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a \n",
      "description of what the prompt is best suited for. You may also revise the original input if you think that revising\n",
      "it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "physics\n",
      "math\n",
      "history\n",
      "computerscience\n",
      "\n",
      "<< INPUT >>\n",
      "What is the capital of France?\n",
      "\n",
      "\n",
      "<< OUTPUT (remember to include the ```json)>>\n",
      "\n",
      "Input to destination chain: \n",
      "{'destination': 'history', 'next_inputs': 'What was the capital of France during the French Revolution?'} \n",
      "<class 'dict'>\n",
      "\n",
      "During the French Revolution, which lasted from 1789 to 1799, the capital of France was Paris. In fact, Paris played a pivotal role in the Revolution, serving as the epicenter of political power, social change, and cultural upheaval.\n",
      "\n",
      "In August 1789, the National Assembly, which had been meeting in Versailles since 1770, moved its seat to Paris, marking a significant shift from the royal court's dominance. The city became the hub of revolutionary activity, with many key events taking place on or near the streets of Paris.\n",
      "\n",
      "The fall of the Bastille on July 14, 1789, is often seen as a symbol of the Revolution's beginning, and it was in Paris that the National Convention, which succeeded the Legislative Assembly, took power in September 1792. The Convention, led by Maximilien Robespierre, implemented radical policies, including the execution of King Louis XVI and Queen Marie Antoinette.\n",
      "\n",
      "The Reign of Terror, which lasted from 1793 to 1794, saw Paris at its most turbulent, with thousands executed for perceived counter-revolutionary activities. However, in September 1795, the fall of Robespierre marked a turning point, and the Thermidorian Reaction led to a more moderate government.\n",
      "\n",
      "Throughout the Revolution, Paris remained the seat of power, and it was there that many key figures, including Napoleon Bonaparte, rose to prominence. The city's influence extended beyond France, shaping European politics and culture for decades to come.\n",
      "\n",
      "In summary, Paris was the capital of France during the French Revolution, serving as a symbol of revolutionary ideals, social upheaval, and ultimately, the rise of Napoleon Bonaparte.\n"
     ]
    }
   ],
   "source": [
    "answer = main_chain.invoke(\"What is the capital of France?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
